{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, re, csv, requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(x):\n",
    "    item = x.get_text().replace('\\n', '').replace('\\u200b', '')\n",
    "    if len(item):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def select_title(soup, selector):\n",
    "    title = soup.select_one(selector)\n",
    "    if title is None:\n",
    "        return None\n",
    "    return title.get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('search_page.html', 'r', encoding='utf-8') as f:\n",
    "    d = f.read()\n",
    "soup = BeautifulSoup(d, \"lxml\")\n",
    "a_tags = soup.find_all(\"a\", attrs={\"class\": \"api_txt_lines\"})\n",
    "blog_links = [a_tag.attrs.get('href') for a_tag in a_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'\n",
    "options = Options()\n",
    "options.add_argument(f\"--user-agent={ua}\")\n",
    "service=Service(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "filename = 'blog_crawl_origin.csv'\n",
    "with open(filename, 'w', encoding='utf-8-sig', newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    # 컬럼 명\n",
    "    columns = \"line, url, site, title, author, created_date, text\".split(', ')\n",
    "    writer.writerow(columns)\n",
    "    line = 0\n",
    "    for i, blog_link in enumerate(blog_links):\n",
    "        if not blog_link.startswith('https://blog.naver.com'): continue\n",
    "        browser.get(blog_link)\n",
    "        browser.switch_to.frame('mainFrame')\n",
    "        soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "        \n",
    "        first_title = select_title(soup, 'div.se-module.se-module-text.se-title-text')\n",
    "        second_title = select_title(soup, 'div.se_editView.se_title')\n",
    "        \n",
    "        if first_title: title = first_title\n",
    "        elif second_title: title = second_title\n",
    "        else: continue\n",
    "        \n",
    "        author = soup.select_one('span.nick').get_text()\n",
    "        created_date = soup.select_one('span.se_publishDate').get_text()\n",
    "        texts = [item.get_text().replace('\\n', '').replace('\\u200b', '') for item in soup.select('div.se-component.se-text') if cut(item)]\n",
    "        text = '\\n'.join(texts)\n",
    "        \n",
    "        if text == '': continue\n",
    "        \n",
    "        record = [line, blog_link, 'naver blog', title, author, created_date, text]\n",
    "        writer.writerow(record)\n",
    "        \n",
    "        line += 1\n",
    "        \n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'blog_crawl_origin.csv'\n",
    "with open(filename, 'r', encoding='utf-8-sig') as f:\n",
    "    lines = csv.reader(f)\n",
    "    with open('blog_crawl.csv', 'w', encoding='utf-8-sig', newline='') as fw:\n",
    "        writer = csv.writer(fw)\n",
    "        writer.writerow(['index', 'created_date', 'text'])\n",
    "        for i, line in enumerate(list(lines)[1:]):\n",
    "            index = line[0]\n",
    "            created_date = line[5]\n",
    "            txt = line[3] + '\\n' + line[6]\n",
    "            writer.writerow([index, created_date, txt])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('scraping_cenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e3930aa049ef73f271c90f85a1ec43d8dd86a95dcb8cc69fef2eee59d31d743"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
